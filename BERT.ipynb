{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT + POS tagging for Toxicity Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import regex\n",
    "import nltk # TODO: learnable POS encoder to add to model\n",
    "from transformers import BertTokenizer\n",
    "from model import CombinedEmbeddingModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import pos_tag\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"ok\")\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# Download NLTK tagger\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Read data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Preprocessing:\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def load_contractions(file_path=\"./contractions.json\"):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)  # Load JSON data\n",
    "        return data\n",
    "    except (json.JSONDecodeError, FileNotFoundError):\n",
    "        return {}\n",
    "\n",
    "contractions = load_contractions()\n",
    "contractions_re = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions.keys()) + r')\\b')\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions_re.sub(lambda x: contractions[x.group(0)], text)\n",
    "\n",
    "def process_dataframe(frame):\n",
    "    frame = frame.dropna(subset=['comment_text'])\n",
    "    frame[\"comment_text\"] = frame[\"comment_text\"].apply(expand_contractions)\n",
    "    return frame\n",
    "\n",
    "TO_REMOVE = '\"()+,-./:;<=>[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "OBSCENITY = '!#$%&*?@'\n",
    "\n",
    "def remove_chars(text):\n",
    "    pattern = f\"[{re.escape(TO_REMOVE)}]\"\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "    pattern = f\"[{re.escape(OBSCENITY)}]\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "df_train = process_dataframe(df_train)\n",
    "df_test = process_dataframe(df_test)\n",
    "df_train['comment_text'] = df_train['comment_text'].str.replace(r'\\bhttp?\\S+\\b', 'link', regex=True)\n",
    "df_test['comment_text'] = df_test['comment_text'].str.replace(r'\\bhttp?\\S+\\b', 'link', regex=True)\n",
    "df_train[\"comment_text\"] = df_train[\"comment_text\"].apply(clean_text)\n",
    "df_test[\"comment_text\"] = df_test[\"comment_text\"].apply(clean_text)\n",
    "df_train[\"comment_text\"] = df_train[\"comment_text\"].apply(remove_chars)\n",
    "df_test[\"comment_text\"] = df_test[\"comment_text\"].apply(remove_chars)\n",
    "\n",
    "# Train test splitting\n",
    "\n",
    "train_text = df_train[\"comment_text\"].astype(str)\n",
    "train_labels = df_train[\"target\"].astype(float)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_text, train_labels, test_size=0.1, random_state=7)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "print(tokenizer.is_fast)\n",
    "\n",
    "# Function for POS tagging\n",
    "def pos_tag_tokens(tokens, tokenizer):\n",
    "    decoded = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    posTags = []\n",
    "    for token in decoded:\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            posTags.append((token, 'SPECIAL'))\n",
    "        else:\n",
    "            word = token.replace('##', '')\n",
    "            posTags.append(pos_tag([word])[0])\n",
    "    return posTags\n",
    "tags = ['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n",
    "taggerToId = {'SPECIAL': 0, 'NN': 1, 'VB': 2, 'JJ': 3, 'RB': 4, 'IN': 5}\n",
    "maxInd = 5\n",
    "for tag in tags:\n",
    "    if tag not in taggerToId:\n",
    "        maxInd += 1\n",
    "        taggerToId[tag] = maxInd\n",
    "def convertToInd(tags, taggerToId):\n",
    "    return [taggerToId.get(tag, 0) for word, tag in tags]\n",
    "\n",
    "# Prepare a tokenizer with batching\n",
    "def batch_tokenize_and_tag(texts, tokenizer, tokType, batch_size=32, max_length=512):\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    pos_tag_list = []\n",
    "    save_dir = f\"Batches/tokens_{tokType}\"\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch with padding and truncation\n",
    "        encoded_batch = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        temp = tf.convert_to_tensor(encoded_batch['input_ids'])\n",
    "        input_ids_list.append(temp)\n",
    "        attention_masks_list.append(tf.convert_to_tensor(encoded_batch['attention_mask']))\n",
    "\n",
    "        # POS tagging (convert tokenized ids to words and tag)\n",
    "        for ids in temp:\n",
    "            tokens = ids.numpy().tolist()\n",
    "            tags = pos_tag_tokens(tokens, tokenizer)\n",
    "            pos_tag_list.append(convertToInd(tags, taggerToId))\n",
    "        print(f\"Sample number {i}\")\n",
    "    # Concatenate everything into tensors\n",
    "    input_ids = tf.concat(input_ids_list, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks_list, axis=0)\n",
    "    pos_tags_tensor = tf.convert_to_tensor(pos_tag_list, dtype=tf.int32)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_masks, \"pos_tags\": pos_tags_tensor}\n",
    "# Tokenize and tag data in batches\n",
    "train_data = batch_tokenize_and_tag(train_texts.tolist()[:204800], tokenizer, \"train\", batch_size=2048) # Only using first 200k samples\n",
    "val_data = batch_tokenize_and_tag(val_texts.tolist()[:50000], tokenizer, \"val\", batch_size=2048) # only using first 50k samples\n",
    "\n",
    "# Output results\n",
    "train_toks = {\"input_ids\": train_data[\"input_ids\"], \"attention_mask\": train_data[\"attention_mask\"]}\n",
    "val_toks = {\"input_ids\": val_data[\"input_ids\"], \"attention_mask\": val_data[\"attention_mask\"]}\n",
    "trainTagInds = train_data[\"pos_tags\"]\n",
    "valTagInds = val_data[\"pos_tags\"]\n",
    "\n",
    "# Start building model \n",
    "\n",
    "num_secondary_embeddings = 46\n",
    "embedding_dim = 16\n",
    "dropout_rate = 0.3\n",
    "model = CombinedEmbeddingModel(num_secondary_embeddings, embedding_dim, dropout_rate)\n",
    "train_labels = tf.convert_to_tensor(train_labels[:204800])\n",
    "val_labels = tf.convert_to_tensor(val_labels[:50000])\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=loss, metrics=metrics)\n",
    "val_ins = (val_toks[\"input_ids\"], val_toks[\"attention_mask\"], valTagInds)\n",
    "train_ins = (train_toks[\"input_ids\"], train_toks[\"attention_mask\"], trainTagInds)\n",
    "model.fit(x=train_ins, y=train_labels, validation_data=(val_ins, val_labels), epochs=10, batch_size = 64, callbacks=[checkpoint], verbose=1)\n",
    "test_text = df_test[\"comment_text\"].astype(str)\n",
    "test_data = batch_tokenize_and_tag(test_text.tolist(), tokenizer, \"test\", batch_size=2048)\n",
    "test_toks = {\"input_ids\": test_data[\"input_ids\"], \"attention_mask\": test_data[\"attention_mask\"]}\n",
    "testTagInds = test_data[\"pos_tags\"]\n",
    "test_ins = (test_toks[\"input_ids\"], test_toks[\"attention_mask\"], testTagInds)\n",
    "def save_submission(final_prediction, test_df):\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df.id,\n",
    "        'prediction': final_prediction\n",
    "    })\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "preds = model.predict(test_ins, verbose=1).flatten()\n",
    "save_submission(preds, df_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
