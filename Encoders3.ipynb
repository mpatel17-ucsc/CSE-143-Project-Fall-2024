{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7effd9aa-c1c0-44b8-b093-13c88e84c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Documents/CSE-143-Project-Fall-2024/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-11 05:12:18.692858: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-11 05:12:18.707808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733893938.726917   79524 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733893938.732401   79524 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 05:12:18.751230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import regex\n",
    "import nltk # TODO: learnable POS encoder to add to model\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import pos_tag\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d9e9bf-9670-4613-825e-66757fe27133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a GPU\n"
     ]
    }
   ],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if len(physical_devices) > 0:\n",
    "#     print(\"We got a GPU\")\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# else:\n",
    "#     print(\"Sorry, no GPU for you...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464018e9-23ef-4676-8d01-026df2b7c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Set memory growth to True for all GPUs (or False if you prefer not to use memory growth)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafe4794-a35b-445a-8385-5b2cc037b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e763b7-c81f-4aa9-bc16-308606789ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb23b46a-1474-437a-838c-c22c2410e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def load_contractions(file_path=\"./contractions.json\"):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)  # Load JSON data\n",
    "        return data\n",
    "    except (json.JSONDecodeError, FileNotFoundError):\n",
    "        return {}\n",
    "\n",
    "contractions = load_contractions()\n",
    "contractions_re = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions.keys()) + r')\\b')\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions_re.sub(lambda x: contractions[x.group(0)], text)\n",
    "\n",
    "def process_dataframe(frame):\n",
    "    frame = frame.dropna(subset=['comment_text'])\n",
    "    frame[\"comment_text\"] = frame[\"comment_text\"].apply(expand_contractions)\n",
    "    return frame\n",
    "\n",
    "TO_REMOVE = '\"()+,-./:;<=>[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "OBSCENITY = '!#$%&*?@'\n",
    "\n",
    "def remove_chars(text):\n",
    "    pattern = f\"[{re.escape(TO_REMOVE)}]\"\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "    pattern = f\"[{re.escape(OBSCENITY)}]\"\n",
    "    return re.sub(pattern, \"\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716a7bfa-5f82-47a9-ace5-005a6364849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79524/3078383111.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame[\"comment_text\"] = frame[\"comment_text\"].apply(expand_contractions)\n"
     ]
    }
   ],
   "source": [
    "df_train = process_dataframe(df_train)\n",
    "df_test = process_dataframe(df_test)\n",
    "df_train['comment_text'] = df_train['comment_text'].str.replace(r'\\bhttp?\\S+\\b', 'link', regex=True)\n",
    "df_test['comment_text'] = df_test['comment_text'].str.replace(r'\\bhttp?\\S+\\b', 'link', regex=True)\n",
    "df_train[\"comment_text\"] = df_train[\"comment_text\"].apply(clean_text)\n",
    "df_test[\"comment_text\"] = df_test[\"comment_text\"].apply(clean_text)\n",
    "df_train[\"comment_text\"] = df_train[\"comment_text\"].apply(remove_chars)\n",
    "df_test[\"comment_text\"] = df_test[\"comment_text\"].apply(remove_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a389a239-bd1f-481b-a3e8-0ef44318937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = df_train[\"comment_text\"].astype(str)\n",
    "train_labels = df_train[\"target\"].astype(float)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_text, train_labels, test_size=0.1, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca713fb-1af3-475a-b913-0b98d1855138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "print(tokenizer.is_fast)\n",
    "\n",
    "# Function for POS tagging\n",
    "def pos_tag_tokens(tokens, tokenizer):\n",
    "    decoded = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    posTags = []\n",
    "    for token in decoded:\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            posTags.append((token, 'SPECIAL'))\n",
    "        else:\n",
    "            word = token.replace('##', '')\n",
    "            posTags.append(pos_tag([word])[0])\n",
    "    return posTags\n",
    "tags = ['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n",
    "taggerToId = {'SPECIAL': 0, 'NN': 1, 'VB': 2, 'JJ': 3, 'RB': 4, 'IN': 5}\n",
    "maxInd = 5\n",
    "for tag in tags:\n",
    "    if tag not in taggerToId:\n",
    "        maxInd += 1\n",
    "        taggerToId[tag] = maxInd\n",
    "# Function to convert tags to indices\n",
    "def convertToInd(tags, taggerToId):\n",
    "    return [taggerToId.get(tag, 0) for word, tag in tags]\n",
    "\n",
    "# Prepare a tokenizer with batching\n",
    "def batch_tokenize_and_tag(texts, tokenizer, batch_size=32, max_length=512):\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    pos_tag_list = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch with padding and truncation\n",
    "        encoded_batch = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,  # Truncate to max length\n",
    "            padding=\"max_length\",  # Pad to max length\n",
    "            max_length=max_length,  # Return tensors for TensorFlow\n",
    "        )\n",
    "        temp = tf.convert_to_tensor(encoded_batch['input_ids'])\n",
    "        input_ids_list.append(temp)\n",
    "        attention_masks_list.append(tf.convert_to_tensor(encoded_batch['attention_mask']))\n",
    "\n",
    "        # POS tagging (convert tokenized ids to words and tag)\n",
    "        for ids in temp:\n",
    "            tokens = ids.numpy().tolist()\n",
    "            tags = pos_tag_tokens(tokens, tokenizer)\n",
    "            pos_tag_list.append(convertToInd(tags, taggerToId))\n",
    "        print(f\"Sample number {i}\")\n",
    "    # Concatenate everything into tensors\n",
    "    input_ids = tf.concat(input_ids_list, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks_list, axis=0)\n",
    "    # print(pos_tag_list)\n",
    "    pos_tags_tensor = tf.convert_to_tensor(pos_tag_list, dtype=tf.int32)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_masks, \"pos_tags\": pos_tags_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34c37c-ee98-422a-a84b-7998b2353a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733894213.662597   79524 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9583 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\n",
      "I0000 00:00:1733894213.663334   79524 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9063 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number 0\n",
      "Sample number 256\n",
      "Sample number 512\n",
      "Sample number 768\n",
      "Sample number 1024\n",
      "Sample number 1280\n",
      "Sample number 1536\n",
      "Sample number 1792\n",
      "Sample number 2048\n",
      "Sample number 2304\n",
      "Sample number 2560\n",
      "Sample number 2816\n",
      "Sample number 3072\n",
      "Sample number 3328\n",
      "Sample number 3584\n",
      "Sample number 3840\n",
      "Sample number 4096\n",
      "Sample number 4352\n",
      "Sample number 4608\n",
      "Sample number 4864\n",
      "Sample number 5120\n",
      "Sample number 5376\n",
      "Sample number 5632\n",
      "Sample number 5888\n",
      "Sample number 6144\n",
      "Sample number 6400\n",
      "Sample number 6656\n",
      "Sample number 6912\n",
      "Sample number 7168\n",
      "Sample number 7424\n",
      "Sample number 7680\n",
      "Sample number 7936\n",
      "Sample number 8192\n",
      "Sample number 8448\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and tag data in batches\n",
    "train_data = batch_tokenize_and_tag(train_texts.tolist(), tokenizer, batch_size=256)\n",
    "val_data = batch_tokenize_and_tag(val_texts.tolist(), tokenizer, batch_size=256)\n",
    "\n",
    "# Output results\n",
    "train_toks = {\"input_ids\": train_data[\"input_ids\"], \"attention_mask\": train_data[\"attention_mask\"]}\n",
    "val_toks = {\"input_ids\": val_data[\"input_ids\"], \"attention_mask\": val_data[\"attention_mask\"]}\n",
    "trainTagInds = train_data[\"pos_tags\"]\n",
    "valTagInds = val_data[\"pos_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e0592-fbcf-425e-99e2-1b5551e99bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensor(tensor, path):\n",
    "    tf.io.write_file(path, tf.io.serialize_tensor(tensor))\n",
    "    return True\n",
    "def load_tensor(path, dataType=tf.int32):\n",
    "    return tf.io.parse_tensor(tf.io.read_file(path), out_type=dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7ffca-67ab-4414-b4e1-f2dfcaa6cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tensor(train_toks[\"input_ids\"], \"Train_toks/train_input_ids\")\n",
    "save_tensor(train_toks[\"attention_mask\"], \"Train_toks/train_attention_mask\")\n",
    "save_tensor(val_toks[\"input_ids\"], \"Test_toks/test_input_ids\")\n",
    "save_tensor(val_toks[\"attention_mask\"], \"Test_toks/test_attention_mask\")\n",
    "save_tensor(trainTagInds, \"Train_toks/train_tag_inds\")\n",
    "save_tensor(valTagInds, \"Test_toks/test_tag_inds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de5cc272-d696-4674-b7b2-55121492e0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 512), dtype=int32, numpy=\n",
       "array([[ 0, 17, 15, ...,  0,  0,  0],\n",
       "       [ 0, 38, 31, ...,  0,  0,  0],\n",
       "       [ 0,  1,  1, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0, 16,  1, ...,  0,  0,  0],\n",
       "       [ 0,  4,  1, ...,  0,  0,  0],\n",
       "       [ 0,  5, 16, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "205d51c9-173d-48a4-be0c-9985ffdbf759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3758701-5358-467a-ad35-1d004af3acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from model import CombinedEmbeddingModel\n",
    "num_secondary_embeddings = 46\n",
    "embedding_dim = 16\n",
    "dropout_rate = 0.3\n",
    "model = CombinedEmbeddingModel(num_secondary_embeddings, embedding_dim, dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170e383-115f-4ff6-a51f-606ce7714a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "val_labels = tf.convert_to_tensor(val_labels)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aa8f31e-d4a7-4573-8795-3c52c90220f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733893437.289493   75522 service.cc:148] XLA service 0x7f3ae800c680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733893437.289513   75522 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "I0000 00:00:1733893437.289516   75522 service.cc:156]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-12-11 05:03:57.687377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1733893437.788395   75522 assert_op.cc:38] Ignoring Assert operator combined_embedding_model_1/tf_bert_model/bert/embeddings/assert_less/Assert/Assert\n",
      "I0000 00:00:1733893438.586403   75522 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-12-11 05:04:01.530990: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1733893442.414209   75522 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742ms/step - auc: 0.5013 - binary_accuracy: 0.0231 - loss: 0.8050    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733893446.959364   75524 assert_op.cc:38] Ignoring Assert operator combined_embedding_model_1/tf_bert_model/bert/embeddings/assert_less/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - auc: 0.4913 - binary_accuracy: 0.0318 - loss: 0.7985 - val_auc: 0.5948 - val_binary_accuracy: 0.4648 - val_loss: 0.6760\n",
      "Epoch 2/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - auc: 0.5284 - binary_accuracy: 0.5891 - loss: 0.6523 - val_auc: 0.5745 - val_binary_accuracy: 0.6484 - val_loss: 0.5708\n",
      "Epoch 3/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - auc: 0.5144 - binary_accuracy: 0.6917 - loss: 0.5457 - val_auc: 0.5452 - val_binary_accuracy: 0.6484 - val_loss: 0.4930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f3d8a30c3b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ins = (val_toks[\"input_ids\"], val_toks[\"attention_mask\"], valTagInds)\n",
    "train_ins = (train_toks[\"input_ids\"], train_toks[\"attention_mask\"], trainTagInds)\n",
    "model.fit(x=train_ins, y=train_labels, validation_data=(val_ins, val_labels), epochs=3, batch_size = 64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9714ca25-78e7-4655-b136-ef598ef9a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733893635.154297   75524 assert_op.cc:38] Ignoring Assert operator combined_embedding_model_1/tf_bert_model/bert/embeddings/assert_less/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738ms/step - auc: 0.5401 - binary_accuracy: 0.6722 - loss: 0.4723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733893641.923856   75522 assert_op.cc:38] Ignoring Assert operator combined_embedding_model_1/tf_bert_model/bert/embeddings/assert_less/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - auc: 0.5402 - binary_accuracy: 0.6725 - loss: 0.4732 - val_auc: 0.5448 - val_binary_accuracy: 0.6484 - val_loss: 0.4858\n",
      "Epoch 2/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - auc: 0.5430 - binary_accuracy: 0.6870 - loss: 0.4730 - val_auc: 0.5361 - val_binary_accuracy: 0.6484 - val_loss: 0.4790\n",
      "Epoch 3/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - auc: 0.5576 - binary_accuracy: 0.7010 - loss: 0.4630 - val_auc: 0.5440 - val_binary_accuracy: 0.6484 - val_loss: 0.4724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f3d8c369220>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine tune BERT \n",
    "model.bert.trainable=True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=loss, metrics=metrics)\n",
    "model.fit(x=train_ins, y=train_labels, validation_data=(val_ins, val_labels), epochs=3, batch_size = 64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fd53f-9bf6-4859-a188-0b60dce7bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"combined_embedding_model_file\")\n",
    "loaded_model = tf.keras.load_model(\"combined_embedding_model_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11038f6-7475-421c-99ec-89156b31ef4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
