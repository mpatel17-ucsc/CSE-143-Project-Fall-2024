{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vectors...\n",
      "Saving Gensim vectors...\n",
      "Done!\n",
      "3526/3526 - 11048s - 3s/step - dense_2_loss: 0.4296 - dense_3_loss: 0.1095 - loss: 0.5391\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 5s/step\n",
      "3526/3526 - 10526s - 3s/step - dense_2_loss: 0.4130 - dense_3_loss: 0.1042 - loss: 0.5172\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 5s/step\n",
      "3526/3526 - 10770s - 3s/step - dense_2_loss: 0.4081 - dense_3_loss: 0.1030 - loss: 0.5111\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 5s/step\n",
      "3526/3526 - 10623s - 3s/step - dense_2_loss: 0.4047 - dense_3_loss: 0.1024 - loss: 0.5070\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 6s/step\n",
      "3526/3526 - 10837s - 3s/step - dense_6_loss: 0.4297 - dense_7_loss: 0.1097 - loss: 0.5393\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 5s/step\n",
      "3526/3526 - 10388s - 3s/step - dense_6_loss: 0.4132 - dense_7_loss: 0.1042 - loss: 0.5173\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 6s/step\n",
      "3526/3526 - 11248s - 3s/step - dense_6_loss: 0.4083 - dense_7_loss: 0.1030 - loss: 0.5113\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 6s/step\n",
      "3526/3526 - 11010s - 3s/step - dense_6_loss: 0.4046 - dense_7_loss: 0.1023 - loss: 0.5070\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Bidirectional, LSTM, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "\n",
    "# Paths for GloVe and Gensim files\n",
    "glove_path = '/kaggle/input/glove-6b-300d-txt/glove.6B.300d.txt'\n",
    "gensim_path = 'glove.6B.300d.gensim'\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_paths = [gensim_path]\n",
    "num_models = 2\n",
    "batch_size = 512\n",
    "lstm_units = 128\n",
    "dense_units = 4 * lstm_units\n",
    "epochs = 4\n",
    "max_sequence_length = 220\n",
    "\n",
    "# Columns for identity and auxiliary information\n",
    "identity_cols = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "auxiliary_cols = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "text_column = 'comment_text'\n",
    "target_column = 'target'\n",
    "char_to_remove = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "\n",
    "# Load and convert GloVe to Gensim format\n",
    "def convert_glove_to_gensim(glove_path, gensim_path):\n",
    "    glove_model = gensim.models.KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
    "    glove_model.save(gensim_path)\n",
    "\n",
    "# Create the embedding matrix from GloVe embeddings\n",
    "def create_embedding_matrix(word_index, embedding_path):\n",
    "    embeddings = KeyedVectors.load(embedding_path, mmap='r')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, index in word_index.items():\n",
    "        for variant in [word, word.lower()]:\n",
    "            if variant in embeddings:\n",
    "                embedding_matrix[index] = embeddings[variant]\n",
    "                break\n",
    "    return embedding_matrix\n",
    "\n",
    "# Define the model architecture\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    input_text = Input(shape=(None,))\n",
    "    embedding_layer = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(input_text)\n",
    "    x = SpatialDropout1D(0.2)(embedding_layer)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "\n",
    "    pooled = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\n",
    "    hidden_layer = Dense(dense_units, activation='relu')(pooled)\n",
    "    hidden_layer = Dense(dense_units, activation='relu')(hidden_layer)\n",
    "    \n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(hidden_layer)\n",
    "    auxiliary_output = Dense(num_aux_targets, activation='sigmoid', name='aux_output')(hidden_layer)\n",
    "\n",
    "    model = Model(inputs=input_text, outputs=[main_output, auxiliary_output])\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics={'main_output': ['accuracy'], 'aux_output': ['accuracy']}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load train and test datasets\n",
    "def load_data():\n",
    "    train_df = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "    return train_df, test_df\n",
    "\n",
    "# Preprocess the data (tokenization and padding)\n",
    "def preprocess_data(train_df, test_df):\n",
    "    x_train_data = train_df[text_column].astype(str)\n",
    "    y_train_data = train_df[target_column].values\n",
    "    y_aux_train_data = train_df[auxiliary_cols].values\n",
    "    x_test_data = test_df[text_column].astype(str)\n",
    "\n",
    "    # Convert target and identity columns to binary (True/False)\n",
    "    for col in identity_cols + [target_column]:\n",
    "        train_df[col] = (train_df[col] >= 0.5).astype(bool)\n",
    "\n",
    "    text_tokenizer = Tokenizer(filters=char_to_remove, lower=False)\n",
    "    text_tokenizer.fit_on_texts(list(x_train_data) + list(x_test_data))\n",
    "\n",
    "    x_train_data = text_tokenizer.texts_to_sequences(x_train_data)\n",
    "    x_test_data = text_tokenizer.texts_to_sequences(x_test_data)\n",
    "    x_train_data = sequence.pad_sequences(x_train_data, maxlen=max_sequence_length)\n",
    "    x_test_data = sequence.pad_sequences(x_test_data, maxlen=max_sequence_length)\n",
    "\n",
    "    return x_train_data, y_train_data, y_aux_train_data, x_test_data, text_tokenizer\n",
    "\n",
    "# Calculate sample weights for the training data\n",
    "def calculate_sample_weights(train_df):\n",
    "    sample_weights = np.ones(len(train_df), dtype=np.float32)\n",
    "    sample_weights += train_df[identity_cols].sum(axis=1)\n",
    "    sample_weights += train_df[target_column] * (~train_df[identity_cols]).sum(axis=1)\n",
    "    sample_weights += (~train_df[target_column]) * train_df[identity_cols].sum(axis=1) * 5\n",
    "    sample_weights /= sample_weights.mean()\n",
    "    return sample_weights\n",
    "\n",
    "# Train and evaluate the model\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate_model(model, x_train_data, y_train_data, y_aux_train_data, sample_weights, x_test_data):\n",
    "    model_predictions = []\n",
    "    model_weights = []\n",
    "\n",
    "    for epoch_index in range(epochs):\n",
    "        # Fit the model for 1 epoch\n",
    "        history = model.fit(\n",
    "            x_train_data,\n",
    "            [y_train_data, y_aux_train_data],\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)]\n",
    "        )\n",
    "\n",
    "        # Print training accuracy for main and auxiliary outputs\n",
    "        main_acc = history.history['main_output_accuracy'][-1]\n",
    "        aux_acc = history.history['aux_output_accuracy'][-1]\n",
    "\n",
    "        print(f\"Epoch {epoch_index + 1}/{epochs}, Main Accuracy: {main_acc}, Auxiliary Accuracy: {aux_acc}\")\n",
    "        \n",
    "        # Save model predictions for the current checkpoint\n",
    "        model_predictions.append(model.predict(x_test_data, batch_size=2048)[0].flatten())\n",
    "        model_weights.append(2 ** epoch_index)\n",
    "\n",
    "    return model_predictions, model_weights\n",
    "\n",
    "# Finalize the predictions\n",
    "def finalize_predictions(model_predictions, model_weights):\n",
    "    final_prediction = np.average(model_predictions, weights=model_weights, axis=0)\n",
    "    return final_prediction\n",
    "\n",
    "# Prepare and save the submission file\n",
    "def save_submission(final_prediction, test_df):\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df.id,\n",
    "        'prediction': final_prediction\n",
    "    })\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Main process\n",
    "def main():\n",
    "    convert_glove_to_gensim(glove_path, gensim_path)\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    train_df, test_df = load_data()\n",
    "    x_train_data, y_train_data, y_aux_train_data, x_test_data, text_tokenizer = preprocess_data(train_df, test_df)\n",
    "\n",
    "    # Calculate sample weights\n",
    "    sample_weights = calculate_sample_weights(train_df)\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    embedding_matrix = np.concatenate(\n",
    "        [create_embedding_matrix(text_tokenizer.word_index, file) for file in embedding_paths], axis=-1\n",
    "    )\n",
    "\n",
    "    # Train models and collect predictions\n",
    "    model_predictions = []\n",
    "    model_weights = []\n",
    "\n",
    "    for model_index in range(num_models):\n",
    "        model = build_model(embedding_matrix, y_aux_train_data.shape[-1])\n",
    "        model_predictions_for_model, model_weights_for_model = train_and_evaluate_model(\n",
    "            model, x_train_data, y_train_data, y_aux_train_data, sample_weights, x_test_data\n",
    "        )\n",
    "        model_predictions.extend(model_predictions_for_model)\n",
    "        model_weights.extend(model_weights_for_model)\n",
    "\n",
    "    # Finalize predictions and save the submission\n",
    "    final_prediction = finalize_predictions(model_predictions, model_weights)\n",
    "    save_submission(final_prediction, test_df)\n",
    "\n",
    "# Run the main process\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
