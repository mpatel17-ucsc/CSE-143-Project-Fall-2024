{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vectors...\n",
      "Saving Gensim vectors...\n",
      "Done!\n",
      "3526/3526 - 11048s - 3s/step - dense_2_loss: 0.4296 - dense_3_loss: 0.1095 - loss: 0.5391\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 5s/step\n",
      "3526/3526 - 10526s - 3s/step - dense_2_loss: 0.4130 - dense_3_loss: 0.1042 - loss: 0.5172\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 5s/step\n",
      "3526/3526 - 10770s - 3s/step - dense_2_loss: 0.4081 - dense_3_loss: 0.1030 - loss: 0.5111\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 5s/step\n",
      "3526/3526 - 10623s - 3s/step - dense_2_loss: 0.4047 - dense_3_loss: 0.1024 - loss: 0.5070\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 6s/step\n",
      "3526/3526 - 10837s - 3s/step - dense_6_loss: 0.4297 - dense_7_loss: 0.1097 - loss: 0.5393\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 5s/step\n",
      "3526/3526 - 10388s - 3s/step - dense_6_loss: 0.4132 - dense_7_loss: 0.1042 - loss: 0.5173\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 6s/step\n",
      "3526/3526 - 11248s - 3s/step - dense_6_loss: 0.4083 - dense_7_loss: 0.1030 - loss: 0.5113\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 6s/step\n",
      "3526/3526 - 11010s - 3s/step - dense_6_loss: 0.4046 - dense_7_loss: 0.1023 - loss: 0.5070\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_input_file = '/kaggle/input/glove-6b-300d-txt/glove.6B.300d.txt'\n",
    "gensim_output_file = 'glove.6B.300d.gensim'\n",
    "\n",
    "def glove_to_gensim(glove_input_file, gensim_output_file):\n",
    "    print(\"Loading GloVe vectors...\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(glove_input_file, binary=False, no_header=True)\n",
    "    print(\"Saving Gensim vectors...\")\n",
    "    model.save(gensim_output_file)\n",
    "    print(\"Done!\")\n",
    "\n",
    "glove_to_gensim(glove_input_file, gensim_output_file)\n",
    "\n",
    "EMBEDDING_FILES = [gensim_output_file]\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "IDENTITY_COLUMNS = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "                    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "\n",
    "# Build the embedding matrix\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = KeyedVectors.load(path, mmap='r')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        for candidate in [word, word.lower()]:\n",
    "            if candidate in embedding_index:\n",
    "                embedding_matrix[i] = embedding_index[candidate]\n",
    "                break\n",
    "    return embedding_matrix\n",
    "\n",
    "# Build the model\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid', name='main_output')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid', name='aux_output')(hidden)\n",
    "\n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "\n",
    "    # Define distinct accuracy metrics for each output using the names of the outputs\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics={'main_output': ['accuracy'], 'aux_output': ['accuracy']}  # Match the layer names here\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Read train and test data\n",
    "train_df = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMNS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)\n",
    "\n",
    "# Convert target and identity columns to boolean values\n",
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "# Calculate sample weights\n",
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "sample_weights /= sample_weights.mean()\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "\n",
    "# Prepare for storing predictions\n",
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "# Train the model\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        # Train the model for 1 epoch and get the history object\n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)]\n",
    "        )\n",
    "        \n",
    "        # Print the accuracy for the primary and auxiliary outputs\n",
    "        # Access accuracy for the first output (main target)\n",
    "        train_accuracy = history.history['main_output_accuracy'][-1]  # Accuracy for the first output\n",
    "        aux_train_accuracy = history.history['aux_output_accuracy'][-1]  # Accuracy for the second output\n",
    "        \n",
    "        print(f\"Epoch {global_epoch + 1}/{EPOCHS}, Train accuracy: {train_accuracy}, Aux Train accuracy: {aux_train_accuracy}\")\n",
    "        \n",
    "        # Save predictions for the checkpoint\n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        weights.append(2 ** global_epoch)\n",
    "\n",
    "# Combine predictions from all models and calculate the average\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "\n",
    "# Create the submission file\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df.id,\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
